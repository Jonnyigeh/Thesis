\documentclass{subfiles}
\begin{document}
%% lina
\section{Linear Algebra}
Before we tackle quantum mechanics itself, we first review the key linear-algebra tools that underpin it. Quantum mechanics is founded on the physics of waves and the language of linear operators, and a solid grasp of these topics will greatly aid our exploration of its more intricate features.
In this thesis, we will be dealing with $n$-dimensional complex vector spaces, expressed in Dirac notation, which is a compact and powerful notation for linear algebra in quantum mechanics.
%% hey paul
\subsection{Dirac notation}
Introduced and named by the famous physicist Paul Dirac in his paper 'A new notation for quantum mechanics' \cite{dirac_1939}, this notation allows for linear algebra in quantum mechanics to be performed in a neat and compact way. Any $n$-dimensional complex vector in the vector space $V$ is represented as a ket, $\ket{\psi}$, and its dual-vector as a bra, $\bra{\psi}$. The inner product of two vectors is then defined as:
\begin{align*}
        \braket{\psi|\phi} = \int \psi^*(x)\phi(x) dx,
\end{align*} 
and any generic vector $\ket{c}$ can be expressed in a basis set $\{\ket{i}\}$ as:
\begin{align}
    \ket{c} = \sum_i c_i \ket{i} = \sum_i \ket{i}\braket{i|c}, 
\end{align}
where $c_i$ are the coefficients of the expansion. This naturally introduces the \emph{completeness relation}, where the following holds:
\begin{equation}
    I = \sum_i \ket{i}\bra{i}\label{eq:completeness_relation},
\end{equation}
where $I$ is the identity operator. This relation guarantees that the basis set $\{\ket{i}\}$ spans the entire vector space $V$.

%% Ops
\subsection{Operators}
An operator is a mathematical object that maps elements from one vector space $V$ to itself (or, more generally, to a new vector space $W$)\cite{berera2021quantum}. In this work, we focus on \emph{linear operators}, i.e mappings $\hat O: V \rightarrow V$ that satisfy the following linearity properties:
\begin{align*}
    \hat O(\alpha \mathbf{x} + \beta\mathbf{y}) = \alpha \hat O\mathbf{x} + \beta \hat O\mathbf{y},
\end{align*}
for any $\mathbf{x}$, $\mathbf{y}$ in $V$ and $\alpha, \beta\in \mathbb{C}$. In the dirac notation then, we can write the following to show how an operator $\hat O$ acts on kets $\ket{i}$ in our space
\begin{align*}
    \hat O\ket{i} = \alpha \ket{i}.
\end{align*}
We can also define the adjoint of this operator, $\hat O^\dagger$, which acts on the dual-vectors, the bras, in the same space
\begin{align*}
    \bra{i}\hat O^\dagger = \alpha^* \bra{i}.
\end{align*}
One of the most useful properties of linear operators, is that given a basis $\{\ket{i}\}$, we can express the operators as matrices, where the matrix elements are as follows
\begin{align*}
    O_{ij} = \bra{i}\hat O\ket{j} = \sum_k\braket{i|k}O_{kj},
\end{align*}
and we can identify the action of the operator on the vector in this basis, as a matrix-vector product
\begin{align*}
    \hat O\ket{i} = \sum_j \ket{j}O_{ji}.
\end{align*}
In quantum mechanics, certain operators play a fundamental role due to their specific mathematical properties \cite{griffiths2018introduction, berera2021quantum}. These operators represent key physical observables and transformations, which we will explore in detail in the quantum mechanics section. However, some of their defining mathematical characteristics are crucial to emphasize:
\begin{itemize}
    \item \textbf{Unitary:} A matrix $U$ is unitary if $U^*U = I$, meaning the inverse of the matrix $U$ is it's complex conjugate. This property of unitary matrices is such that they preserve the inner product of vectors, e.g $\braket{\psi|\phi} = \braket{U\psi|U\phi}$, and the unitary matrices also preserve the vector norm. Unitary matrices are used to represent time-evolution operators in quantum mechanics.
    \item \textbf{Hermitian:} A matrix $U$ is hermitian if $U = U^\dagger$, meaning the matrix is equal to its complex conjugate transpose\footnote{represented by a $\dagger = *^T$}. This is a special case of self-adjoint operators, living in a \emph{complex} Hilbert space, where the eigenvalues are real and the eigenvectors are orthogonal. These matrices are used to represent physical observables in quantum mechanics, like the Hamiltonian operator, which represents the total energy of a quantum mechanical system.
\end{itemize}
%% mr hilbert
\subsection{Hilbert spaces}\label{sec:Hilbert_space}
The concept of a Hilbert space is fundamental to quantum mechanics, as it provides the mathematical framework for which quantum states and operators are defined. A Hilbert space is a generalization of Euclidiean space, which allows for linear algerba and calculus to be applied to infinite-dimensional spaces. More formally, a Hilbert space is a complex complete inner product space, which means that it adhers to the following properties \cite{griffiths2018introduction, berera2021quantum}: 
\begin{itemize}
    \item \textbf{Completeness:} Every Cauchy sequence $\{x_n\}$ in the space $V$ converges to a limit in the space. Specifically, $V$ is complete if:
    \begin{equation}
        \forall \epsilon > 0,\quad \exists N \in \mathbb{N} : |x_n - x_m| < \epsilon, \quad \forall n,m > N.
    \end{equation}
    \item \textbf{Positivity:} The inner product of a vector with itself is always positive, and zero if and only if the vector is the zero vector. That is:
    \begin{equation}
        \braket{\phi|\psi}\geq 0, \quad \text{and} \braket{\phi|\phi} = 0 \iff \ket{\phi} = \ket{0}. 
    \end{equation}
    \item \textbf{Multiplicativity:} The inner product is linear in the second argument and conjugate linear in the first argument, meaning:
    \begin{equation}
        \braket{\beta\phi|\alpha_1\psi_1 + \alpha_2\psi_2} = \alpha_1\beta^*\braket{\phi|\psi_1} + \alpha_2\beta^*\braket{\phi|\psi_2}. 
    \end{equation}
\end{itemize}
The choice for conjugate linearity in the first agrument is by convention in many physics textbooks. Vectors living in Hilbert space is in quantum mechanics often coined state-vectors. \\ As we mentioned, a Hilbert space is an inner product space, and we define the inner product on the space $V$ as
\begin{align*}
    \braket{\phi|\psi} = \int \phi^* \psi dx,
\end{align*}
for the complex-valued, continuous, state vectors $\ket{\phi}, \ket{\psi} \in V$. \\ We can use this inner product to define \emph{orthogonality} in the Hilbert space
\begin{align*}
    \braket{\psi_i|\psi_j} = \delta_{ij},
\end{align*}
where $\delta_{ij}$ is the Kronecker delta. This orthogonality relation is crucial in quantum mechanics, as it allows us to define a basis set of vectors that are orthogonal to each other. As elegant as this framework may be, the challenge of quantum many-body systems lies in the exponential scaling of the Hilbert space with the number of particles and degrees of freedom. For a system of $N$ particles, with $d$ degrees of freedom, the dimension of the Hilbert space is $d^N$, which quickly becomes intractable for classical computers. This rapid growth makes exact solutions for interacting many-body systems computationally infeasible, even in low-dimensional systems \cite{helgaker2013molecular, szabo1996modern}.

%% BASIS SETS
\subsection{State representation}\label{sec:state_representation}
The representation of state-vectors in a Hilbert space is not unique, and we are free to choose basis sets to express the vectors that span our Hilbert space (or any space for that matter), as long as the basis of choice spans the Hilbert space \cite{griffiths2018introduction, berera2021quantum}. This is a powerful tool, as it allows us to choose the basis that best suits the problem at hand, and clever choices can simplify problems significantly. The flexibility in choosing a basis set is central to both theoretical insights and computational efficiency when solving quantum mechanical problems \cite{helgaker2013molecular, szabo1996modern}. \\

Many modern many-body methods are built upon this principle, where the choice of basis set can make the difference between a computationally infeasible problem and a tractable one. For example, by exploting symmetry, localization, or energy scales, we can often reduce the dimensionality of the problem and focus on the most relevant degrees of freedom. The choice of basis is not just a mathematical convenience, it can also yield physical insights into system's behaviour. For instance, in quantum chemistry, the choice of basis set can reflect the underlying symmetries of the molecular structure, leading to a more efficient representation of the wavefunction and its properties. Similarly, in condensed matter physics, the choice of basis can be guided by the crystal structure or electronic correlations present in the material \cite{baroni2001phonons, kittel2018introduction}. \\

Furthermore, by finding a basis that diagonalizes the Hamiltonian matrix, we can directly solve for the energy eigenvalues and eigenstates. This principle underlies many techniques, such as using harmonic oscillator eigenfunctions for bound systems or plane waves for periodic potentials \cite{griffiths2018introduction, kittel2018introduction}. The search for an optimal basis is often guided by the specific features of the system under study, such as symmetry or localization, making it a central consideration in both theoretical and computational approaches to quantum mechanics. 

Because no single basis is optimal for every task at hand, we are often required to re-express states and operators in different bases during calculations, depending on the problem and what we are currently studying. 

The transformation between two basis sets $\{\ket{i}\}$ and $\{\ket{\alpha}\}$ is given by the unitary transformation matrix $U$:
\begin{align*}
    \ket{\alpha} = \sum_i U_{\alpha i}\ket{i},
\end{align*}
where the matrix elements of $U$ are given by the inner product of the basis vectors:
\begin{align*}
    U_{\alpha i} = \braket{\alpha|i},
\end{align*}
here assuming the basis sets are complete, meaning they obey the completeness relation \ref{eq:completeness_relation}:
\begin{align*}
    \sum_i \ket{i}\bra{i} = \sum_\alpha \ket{\alpha}\bra{\alpha} = I.
\end{align*}
This relation allows the insertion of the identity operator in calculations, to expand state-vectors and operators in the basis of our choice. This underpins many mathematical tools in quantum mechanics, from the spectral decomposition of operators \footnote{See for example chapter 5.5 in \cite{strang2000linear}} to perturbation-theory expansions \cite{griffiths2018introduction, berera2021quantum, sakurai1986modern}. \\


We can also study how the matrix representation of an operator change under a basis transformation between complete orthonormal basis sets \cite{szabo1996modern}.
Assume that $\hat O$ is an operator in the basis $\{\ket{i}\}$, and $\hat O'$ is the same operator expressed in the basis $\{\ket{\alpha}\}$, then the transformation between the two operators is given by:
\begin{align*}
    O'_{\alpha\beta} = \braket{\alpha|\hat O|\beta} = \sum_{ij}\braket{\alpha|i}\bra{i}\hat O\ket{j}\braket{j|\beta} = \sum_{ij}U_{\alpha i}O_{ij}U^\dagger_{j\beta}.
\end{align*}
We have the following transformation rule for operators under a basis transformation:
\begin{align*}
    \hat O' &= U\hat OU^\dagger, \\
    \hat O &= U^\dagger\hat O'U,
\end{align*}
which allows us to freely move between coordinate, momentum, energy or grid-based representations as needed.
\subsection{Basis sets}\label{sec:basis_set}
As we have mentioned, the choice of basis is essential for controlling error, even though there is no uniquely 'correct' basis and one must often settle on a convenient or computationally efficient option. Any complete set of orthonormal vectors can be used to express our state-vectors, and there are numerous choices available, each with its own advantages and disadvantages. The choice can greatly affect computational efficiency, numerical stability and the physical insights gained from the calculations. We will briefly outline some of the most common families of basis sets, and their usages before we present our choice in this thesis:
\begin{itemize}
    \item \textbf{Discrete Variable Representation (DVR):} A grid-based approach that discretizes the continuous space into a finite set of points, with one basis function peaked at each grid point. This method makes coordinate operators diagonal in the basis, retaining analytical expressions for the kinetic energy operator making it both efficient and accurate for smooth potentials \cite{light1985generalized}. 
    \item \textbf{Harmonic oscillator basis:} A set of orthonormal functions that are solutions to the quantum harmonic oscillator (QHO) problem. As such, these are useful for bound systems exhibiting oscillatory behaviour, such as vibrational modes of molecules and quantum dot systems \cite{helgaker2013molecular}.
    \item \textbf{Plane wave basis:} The natural choice for representing periodic systems, such as crystals, and is often employed in condensed matter physics. Their diagonalization of the kinetic energy operators makes them particularly useful for systems with translational symmetry, such as electrons in a crystal lattice \cite{kittel2018introduction}.
    \item \textbf{Gaussian-type orbitals (GTOs):} Common choice for initial approximations in quantum chemistry, particularly in the Hartree-Fock method. GTOs are computationally efficient, and reflect the atomic symmetries of atoms, making them suitable for molecular systems \cite{szabo1996modern}.
    \item \textbf{Energy eigenfunctions:} The eigenfunctions of a chosen reference Hamiltonian (such as the QHO)form an orthonormal set that diagonalizes the Hamiltonian operator. For example, the static or unperturbed system Hamiltonian can be used to construct a basis set of energy eigenfunctions, and using these states makes unperturbed time-evolution trivial, as the time-evolution operator is diagonal in this basis. This is a common approach in quantum chemistry and condensed matter physics. The downside is that for many-body systems, the number of states required to accurately express the wavefunction grow exponentially, and computing the eigenstates of the Hamiltonian becomes intractable \cite{helgaker2013molecular, szabo1996modern}.
\end{itemize}

We focus on the Sinc-DVR basis \cite{colbert1992novel} for the analysis done in our thesis work, due to its simple structure and wide applicability to problems with smooth, localized potentials. The Sinc-DVR basis functions form an orthonormal set on a uniformly spaced spatial grid, and exhibit Kronecker delta-like behaviour at the grid points, making them especially suited for representing localized states and sparse Hamiltonians, such as our bounded Morse double well system. For some analyses we will also use the energy eigenfunctions of the unperturbed system Hamiltonian, mostly to validate and compare results obtained in the Sinc-DVR basis.
\\ The Sinc-DVR basis funtions are mathematically defined as
\begin{align}
    \phi_n(x) = \frac{1}{\sqrt{N}} \text{sinc}\left(\frac{x - x_n}{\Delta x}\right),\label{eq:sinc_dvr}
\end{align}
where $N$ is the number of grid points, $x_n$ is the $n$th grid point, and $\Delta x$ is the grid spacing. The Sinc function is defined as
\begin{align*}
    \text{sinc}(x) = \begin{cases}
        \frac{\sin(\pi x)}{\pi x} & \text{if } x \neq 0, \\
        1 & \text{if } x = 0.
    \end{cases}
\end{align*}
These basis functions are orthonormal, meaning that they satisfy the following relation:
\begin{align*}
    \int_{-\infty}^{\infty} \phi_n(x) \phi_m(x) dx = \delta_{nm},
\end{align*}
where $\delta_{nm}$ is the Kronecker delta function. This orthonormality property is crucial for ensuring that the basis functions can be used to represent quantum states accurately, and results in the Hilbert space being spanned by the basis functions. We shall in later sections discuss further on how we chose to implement the Sinc-DVR basis in our code, by number of grid points, grid spacing etc. 

In the Discrete Variable Representation, the kinetic energy operator is expressed in matrix form using a grid-basis. In atomic units, this operator is given by
\begin{align*}
    T = -\frac{1}{2} \frac{d^2}{dx^2} ,
\end{align*}
and the Sinc-DVR function defined on a uniform grid as in \eqref{eq:sinc_dvr}. The kinetic energy operator matrix elements are then given by
\begin{align*}
    T_{nm} = \braket{\phi_n|-\frac{1}{2} \frac{d^2}{dx^2}|\phi_m}.
\end{align*}
Using the known analytical solution of these integrals with the sinc function properties (see for example Appendix A in \cite{colbert1992novel}), these matrix elements take the form 
\begin{align}
    T_{nm} = \begin{cases}
        -\frac{\pi^2}{6\Delta x^2} & \text{if } n = m, \\
        \frac{(-1)^{(n-m)}}{\Delta x^2(n - m)^2} & \text{if } n \neq m.
    \end{cases}\label{eq:sinc_dvr_kinetic}
\end{align}
This provides us with a simple matrix expression for the Sinc-DVR Hamiltonian that is easily implemented numerically, as it removes the need for numerical differentiation. 

To compute the Coulomb interaction matrix elements in the Sinc-DVR basis, we start from the general form of the Coulomb interaction between two particles with the shielded Coulomb potential:
\begin{align*}
    V_{nmpq} = \int \int \phi_n(x_1)\phi_m(x_2){\frac{\alpha}{|x_1 - x_2| + a^2}}\phi_p^*(x_1)\phi_q^*(x_2) dx_1 dx_2,
\end{align*}
where $\alpha$ is the Coulomb constant, and $a$ is a small shielding parameter to avoid singularities when the particles are very close together. In the DVR basis, this integral becomes a double sum over the grid points with weight $\Delta x^2$. Since each basis function is peaked at a grid point, it satisfies the delta-like property
\begin{align*}
    \phi_n(x_i) = \frac{1}{\sqrt{\Delta x}}\delta_{n,i}.
\end{align*}
Inserting this property into the 4-index two-body integral all four indices collapse via the Kronecker delta. One can then show that the interaction matrix elements in the Sinc-DVR basis are given by
\begin{align*}
    V_{nmpq} = \sum_{i,j}\frac{1}{\Delta x^2}\delta_{n,i}\delta_{m,j}\frac{\alpha}{|x_i - x_j| + a^2}\delta_{p,i}\delta_{q,j} \Delta x^2.
\end{align*}
where we have assumed a uniform grid spacing $\Delta x_1 = \Delta x_2 = \Delta x$. This gives us the final $N\times N$ interaction matrix elements
\begin{equation}
    V_{nm} = \frac{\alpha}{|x_n - x_m| + a^2}\label{eq:sinc_dvr_coulomb}
\end{equation}
and it is clear that the interaction matrix is sparse in the Sinc-DVR basis, since the basis functions are highly localized at the grid points. This sparsity significantly reduces the computational cost of evaluating the interaction elements as they are often zero, or negligible. As a result, the Sinc-DVR approach is particularly well-suited for systems with local interactions and enables efficient numerical simulations of quantum systems with large basis sets \cite{light2000discrete}.

\end{document}