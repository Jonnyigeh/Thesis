\documentclass{subfiles}
\begin{document}
%% lina
\section{Linear Algebra}
Before we dig in to our main course, quantum mechanics, an appetizer is in order. We should revisit some basic linear algebra to set the tone of what we will see further on in this section.
In short terms, quantum mechanics is built upon the physics of waves and linear algebra, and a strong fundation in these will let us gain a deeper understanding of the intricacies within quantum mechanics.
In this thesis, we will be dealing with n-dimensional complex vector spaces, expressed in Dirac notation, which is a compact and powerful notation for linear algebra in quantum mechanics.
%% hey paul
\subsection{Dirac notation}
Introduced and named by the famous physicist Paul Dirac in his paper 'A new notation for quantum mechanics' \cite{dirac_1939}, this notation allows for linear algebra in quantum mechanics to be performed in a neat and compact way. Any $n$-dimensional complex vector in the vector space $V$ is represented as a ket, $\ket{\psi}$, and its dual-vector as a bra, $\bra{\psi}$. The inner of two vectors is then defined as:
\begin{align*}
        \braket{\psi|\phi} = \int \psi^*(x)\phi(x) dx
\end{align*} 
and any generic vector $\ket{i}$ can be written in terms of a basis set $\{\ket{i}\}$ as:
\begin{align}
    \ket{c} = \sum_i c_i \ket{i} = \sum_i \ket{i}\braket{i|c} 
\end{align}
where $c_i$ are the coefficients of the expansion. This naturally introduces the completeness relation, where the following holds:
\begin{equation}
    I = \sum_i \ket{i}\bra{i}
\end{equation}
where $I$ is the identity operator. This relation guarantees that the basis set $\{\ket{i}\}$ spans the entire vector space $V$.

%% Ops
\subsection{Operators}
A linear operator is a mathematical object that maps elements from one vector space $V$ to another space $W$. Meaning it is a mapping $\hat O: V \rightarrow W$, and has the following linearity properties:
\begin{align*}
    \hat O(\alpha \mathbf{x} + \beta\mathbf{y}) = \alpha \hat O\mathbf{x} + \beta \hat O\mathbf{y}
\end{align*}
for any $\mathbf{x}$, $\mathbf{y}$ in $V$ and $\alpha, \beta\in \mathbb{C}$. In the dirac notation then, we can write the following to show how an operator $\hat O$ acts on kets $\ket{i}$ in our space
\begin{align*}
    \hat O\ket{i} = \alpha \ket{j}
\end{align*}
We can also define the adjoint of this operator, $\hat O^\dagger$, which acts on the dual-vectors, the bras, in the same space
\begin{align*}
    \bra{i}\hat O^\dagger = \alpha^* \bra{j}
\end{align*}
One of the most useful properties of linear operators, is that given a basis $\{\ket{i}\}$, we can express the operators as matrices, where the matrix elements are as follows
\begin{align*}
    O_{ij} = \bra{i}\hat O\ket{j} = \sum_k\braket{i|k}O_{kj}
\end{align*}
and we can identify the action of the operator on the vector in this basis, as a matrix-vector product
\begin{align*}
    \hat O\ket{i} = \sum_j \ket{j}O_{ji}
\end{align*}
In quantum mechanics, certain operators play a fundamental role due to their specific mathematical properties. These operators represent key physical observables and transformations, which we will explore in detail in the quantum mechanics section. However, some of their defining mathematical characteristics are crucial to emphasize:
\begin{itemize}
    \item \textbf{Unitary:} A matrix $U$ is unitary if $U^*U = I$, meaning the inverse of the matrix $U$ is it's complex conjugate. This property of unitary matrices is such that they preserve the inner product of vectors, e.g $\braket{\psi|\phi} = \braket{U\psi|U\phi}$, and the unitary matrices also preserve the vector norm. Unitary matrices are used to represent time-evolution operators in quantum mechanics.
    \item \textbf{Hermitian:} A matrix $U$ is hermitian if $U = U^\dagger$, meaning the matrix is equal to its complex conjugate transpose\footnote{represented by a $\dagger = *^T$}. This is a special case of self-adjoint operators, living in a \emph{complex} Hilbert space, where the eigenvalues are real and the eigenvectors are orthogonal. These matrices are used to represent physical observables in quantum mechanics, like the Hamiltonian operator, which represents the total energy of a quantum mechanical system.
\end{itemize}
%% mr hilbert
\subsection{Hilbert spaces}\label{sec:Hilbert_space}
The concept of a Hilbert space is fundamental to quantum mechanics, as it provides the mathematical framework for which quantum states and operators are defined. A Hilbert space is a generalization of Euclidiean space, which allows for linear algerba and calculus to be applied to infinite-dimensional spaces. More formally, a Hilbert space is a complex complete inner product space, which means that it adhers to the following properties: \textcolor{red}{should add some references to the properties of Hilbert spaces, and maybe an illustration? could be cool}
\begin{itemize}
    \item \textbf{Completeness:} Every Cauchy sequence $\{x_n\}$ in the space $V$ converges to a limit in the space. Specifically, $V$ is complete if:
    \begin{equation}
        \forall \epsilon > 0, \exists N \in \mathbb{N} : |x_n - x_m| < \epsilon, \forall n,m > N
    \end{equation}
    \item \textbf{Positivity:} The inner product of a vector with itself is always positive, and zero if and only if the vector is the zero vector. That is:
    \begin{equation}
        \braket{phi}{psi}\geq 0 \text{and} \braket{\phi|\phi} = 0 \iff \ket{\phi} = \ket{0} 
    \end{equation}
    \item \textbf{Multiplicativity:} The inner product is linear in the second argument and conjugate linear in the first argument, meaning:
    \begin{equation}
        \braket{\beta\phi|\alpha_1\psi_1 + \alpha_2\psi_2} = \alpha_1\beta^*\braket{\phi|\psi_1} + \alpha_2\beta^*\braket{\phi|\psi_2} 
    \end{equation}
\end{itemize}
The choice for conjugate linearity in the first agrument is by convention in many physics textbooks. Vectors living in Hilbert space is in quantum mechanics often coined state-vectors. \\ As we mentioned, a Hilbert space is an inner product space, and we define the inner product on the space $V$ as
\begin{align*}
    \braket{\phi|\psi} = \int \phi^* \psi dx
\end{align*}
for the complex-valued, continuous, state vectors $\ket{\phi}, \ket{\psi} \in V$. \\ We can use this inner product to define \emph{orthogonality} in the Hilbert space
\begin{align*}
    \braket{\psi_i|\psi_j} = \delta_{ij}
\end{align*}
where $\delta_{ij}$ is the Kronecker delta. This orthogonality relation is crucial in quantum mechanics, as it allows us to define a basis set of vectors that are orthogonal to each other. As elegant as this framework may be, the challenge of quantum many-body systems lies in the exponential scaling of the Hilbert space with the number of particles and degrees of freedom. For a system of $N$ particles, with $d$ degrees of freedom, the dimension of the Hilbert space is $d^N$, which quickly becomes intractable for classical computers. This rapid growth makes exact solutions for interacting many-body systems computationally infeasible, even in low-dimensional systems. 

%% BASIS SETS
\subsection{Quantum mechanical bases}\textcolor{red}{we should move this section?}
The representation of state-vectors in a Hilbert space is not unique, and we are free to choose basis sets to express the vectors that spans our Hilbert space (or any space for that matter), as long as the basis of choice spans the Hilbert space\textcolor{red}{source?}. This is a powerful tool in quantum mechanics, as it allows us to choose the basis that best suits the problem at hand, and clever choices can simplify problems significantly. The flexibility in choosing a basis set is central to both theoretical insights and computational efficiency when solving quantum mechanical problems. \\

Many modern many-body methods are built upon this principle, where the choice of basis set can make the difference between a computationally infeasible problem and a tractable one. For example, by exploting symmetry, localization, or energy scales, we can often reduce the dimensionality of the problem and focus on the most relevant degrees of freedom. The choice of basis is not just a mathematical convenience, it can also yield physical insights into th systems behaviour. For instance, in quantum chemistry, the choice of basis set can reflect the underlying symmetries of the molecular structure, leading to a more efficient representation of the wavefunction and its properties. Similarly, in condensed matter physics, the choice of basis can be guided by the crystal structure or electronic correlations present in the material. \textcolor{red}{Need souces for this}\\ \\

Furthermore, by finding a basis that diagonalizes the Hamiltonian matrix, we can directly solve for the energy eigenvalues and eigenstates. This principle underlies many techniques, such as using harmonic oscillator eigenfunctions for bound systems or plane waves for periodic potentials\textcolor{red}{souces?}. The search for an optimal basis is often guided by the specific features of the system under study, such as symmetry or localization, making it a central consideration in both theoretical and computational approaches to quantum mechanics. In this thesis, we will frequently use both the energy eigenbasis (i.e the eigenfunctions of the system Hamiltonian) and a grid-based Discrete Variable Representation (DVR) basis. These choices will be introduced and justified further on in later sections, particularly in the context of quantum mechanics (Section \ref{sec:quantum_mechanics}) and numerical implementations (Section \ref{sec:general_study})\textcolor{red}{fix references}
\\ \\ 

Due to the flexible nature of quantum bases, we are often required to re-express our system in various bases, depending on the problem and what we are currently studying. Often, we will have to chang our bases during the course of our calculations and we thus require a mathematical framework to transform between the bases. 

The transformation between two basis sets $\{\ket{i}\}$ and $\{\ket{j}\}$ is given by the unitary transformation matrix $U$:
\begin{align*}
    \ket{\alpha} = \sum_i U_{\alpha i}\ket{i}
\end{align*}
where the matrix elements of $U$ are given by the inner product of the basis vectors:
\begin{align*}
    U_{\alpha i} = \braket{\alpha|i}
\end{align*}
here assuming the basis sets are complete, meaning the following holds:
\begin{align*}
    \sum_i \ket{i}\bra{i} = \sum_\alpha \ket{\alpha}\bra{\alpha} = I
\end{align*}
This is powerful, and we can also investigate how the matrix representation of an operator change under a basis transformation between complete orthonormal basis sets.\cite{szabo1996modern} \\
Assume that $\hat O$ is an operator in the basis $\{\ket{i}\}$, and $\hat O'$ is the same operator expressed in the basis $\{\ket{\alpha}\}$, then the transformation between the two operators is given by:
\begin{align*}
    O'_{\alpha\beta} = \braket{\alpha|\hat O|\beta} = \sum_{ij}\bra{\alpha}\ket{i}\bra{i}\hat O\ket{j}\bra{j}\ket{\beta} = \sum_{ij}U_{\alpha i}O_{ij}U^\dagger_{j\beta}
\end{align*}
And thus, we have the following transformation rule for operators under a basis transformation:
\begin{align*}
    \hat O' &= U\hat OU^\dagger \\
    \hat O &= U^\dagger\hat O'U
\end{align*}

\end{document}