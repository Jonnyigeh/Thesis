\documentclass{subfiles}
\begin{document}
\section{Hartree-Fock}
As mentioned in the theory section\ref{sec:HF_theory}, the Hartree-Fock method is an iterative method to obtain optimal basis functions (single particle orbitals) that would minimize the energy, and by the variational method, converge towards the true ground state energy. This is inherently an approximaton, where we imagine that the electrons occupy the lowest possible single-particle orbitals, and it has been proven many times that this is a rather accurate approximation for many systems. \textcolor{red}{(Cite some sources here)}.
We will now outline the method without much information, before diving into each step in more detail and provide insights into the computational aspects of the method.
\begin{itemize}
    \item Construct an initial guess for the single-particle orbitals, $\{\phi_i\}$, often using atomic orbitals or other basis sets.
    \item Diagonalize the Fock matrix to obtain a new set of orbitals, $\{\phi_i\}$, and calculate Hartree-Fock energy, $E_{HF}$.
    \item Repeat the process until the energy converges, i.e. the change in energy between iterations is below a certain threshold.
    \item Calculate the total energy of the system, and the electron density, and use this to calculate other properties of the system.
\end{itemize}
The first step in the Hartree-Fock procedure is to define an initial ansatz for the trial wavefunction, which is typically represented as a Slater determinant, as discussed earlier. This ansatz requires an initial choice of basis set, which plays a crucial role in the overall success of the method. The initial basis set serves as a starting point for constructing orbitals and significantly influences the convergence of the self-consistent field (SCF) procedure.\\  
\\ Choosing an appropriate basis set is a nuanced and complex task. A well-chosen basis can simplify numerical calculations and accelerate convergence, whereas a poor choice may lead to slow convergence or even failure to converge. 
\\ \\


There are a multitude of basis sets to choose from, and the choice is guided by the nature of the system. For instance, in quantum dots systems with strong confinement, the quantum harmonic oscillator basis sets are often used to great success\cite{Yuan_2017}. A different procedure to using pre-defined basis sets, is to solve the Schrödinger equation for the non-interacting system, and use these single-particle orbitals as the initial basis set. The latter may often yield quicker convergence due to the functions being specifically tailored for the potential, but at the cost of more computational resources, and in some cases may not even be possible. 
\\
\\ With the initial basis, we construct the Fock matrix, which is a matrix representation of the Fock operator in the basis of the single-particle orbitals as seen in section \ref{sec:HF_theory}. The Fock matrix is given by
\begin{align}
    F_{pq} = h_{pq} + \sum_{i,j}^N u_{piqj}\rho_{ij} \label{eq:fock_matrix}
\end{align}
where $\rho_{ij}$ is the density matrix formed from the eigenvectors of the anti-symmetrized system hamiltonian (\textcolor{red}{Dette bør nok nevnes i seksjon 1.2.1 Hartree-Fock, og refereres til hvordan interaksjonleddet gjørs anti-symmtrisk $u = u_ijab - u_ijba$}). The following snippet 
\begin{lstlisting}[language=Python]
h = ...
u = ...
def fill_fock_matrix(C):
    fock = np.zeros(h.shape, dtype=np.complex128)
    density_matrix = np.zeros((h.shape[0],h.shape[0]), dtype=np.complex128)
    for i in range(n_particles):
        density_matrix += np.outer(C[:, i], np.conj(C[:, i]))
    fock = np.einsum('ij, aibj->ab', density_matrix, u, dtype=np.complex128)        # Compute the two-body operator potential
    fock += h                                                                       # Add the one-body operator hamiltonian

    return fock
\end{lstlisting}

\textcolor{red}{not finished}


%%% HARTREE METHOD
\section{Bipartite Hartree}
As presented in section \ref{sec:bipartite_H}, the Hartree method is also a self-consistent field method, where we solve the coupled eigenvalue equations for the two subsystems (particles) iteratively. The method is very similar to the Hartree-Fock method, and the main difference is that we do not include an exchange term and the wavefunction itself a single hartree product state. The general method is as follows:
\begin{itemize}
    \item Construct an initial guess for the single-particle orbitals, $\{\chi_i\}$.
    \item Diagonalize the Hartree matrix to obtain a new set of orbitals, $\{\phi_i\}$, and calculate the energy.
    \item Repeat the process until the energy converges, i.e. the change in energy between iterations is below a certain threshold.
    \item Calculate the total energy of the system, and the electron density, and use this to calculate other properties of the system.
\end{itemize}
The way we solve this SCF procedure is by constructing the Hartree matrix, which is a matrix representation of the Hartree operator in the basis of the single-particle orbitals. \\ \\
We make an initial ansatz for th single-particle orbitals by constructing the Fock matrix in \eqref{eq:fock_matrix} without the interaction term, and diagonalize the matrix to obtain the new set of orbitals, one set for each subsystem. The initial Fock matrix is thus constructed as
\begin{align*}
    f_{\alpha\beta}^{M(0)} = h_{\alpha\beta}^M
\end{align*}
where $M$ is the subsystem index, and $\alpha, \beta$ are the basis functions. This yields our initial set of orbitals, but does not include any interaction between the two subsystems. We then, in the next iterations construct new Fock matrices, and diagonalize them to obtain new orbitals. The new Fock matrix for the i'th iteration is constructed as
\begin{align*}
    f_{\alpha\beta}^{M(i)} = h_{\alpha\beta}^M + \sum_{i}^{N} u_{\alpha\gamma\beta\delta}^{M} \rho_{\gamma\delta}^{M}
\end{align*}
where the density matrix $\rho_{\gamma\delta}^{M}$ is constructed from the previous set of orbitals, and used to calculate the interaction term \emph{for subsystem $M$ specifically}. This differs from Hartree-Fock where the interaction term is calculated from the total density matrix \eqref{eq:hf_equations}. In the following code snippet the density matrix is not used directly, but an equivalent einstein summation is performed, following the equations \eqref{eq:bipartite_hartree} in section \ref{sec:bipartite_H}.
\begin{lstlisting}[language=Python]
def construct_fock_matrices(self, h_l, h_r, u_lr, c_l, c_r):
    return (
        h_l + np.einsum('j, ijkl, l -> ik', c_r[:,0].conj(), u_lr, c_r[:,0]),
        h_r + np.einsum('i, ijkl, k -> jl', c_l[:,0].conj(), u_lr, c_l[:,0]),
    )
def diagonalize_fock_matrices(self, f_l, f_r):
    eps_l, c_l = scipy.linalg.eigh(f_l, subset_by_index=[0, self.num_basis_l - 1])
    eps_r, c_r = scipy.linalg.eigh(f_r, subset_by_index=[0, self.num_basis_r - 1])
    return eps_l, c_l, eps_r, c_r
\end{lstlisting}
With these two functions we set up and iterative loop that repeatedly constructs the Fock matrices, diagonalizes them, and constructs new Fock matrices until the energy converges within a pre-set threshold for the two subsystems, or we reach a maximum number of iterations (and have divergence). We use the \texttt{scipy.linalg}\cite{2020SciPy-NMeth} package and it's hermitian eigensolver \texttt{eigh} to diagonalize the Fock matrices, selecting the subset of functions we wish to extract, corresponding to the number of available basis functions in each subsystem. \textcolor{red}{(Not finished, need to write more about the convergence criteria, and the iterative loop perhaps?)}
\section{Study of indistinguishability}
One of the core features of our system - and the foundation of our qubit design -is that the two particles confined in our potential trap act as \emph{distinguishable} particles. As we are looking to trap electrons, indistinguishable particles, we are dealing with fermions, and the Pauli exclusion principle states clearly that two fermions cannot occupy the same quantum state. This fundamental principle would normally prevnt us from constructing product states, as the quantum state of any fermionic system must be anti-symmetric under particle exchange.
\\ However, we are assuming that the potential trap is constructed such that there are minimal correlations between the two particles, and that the wavefunction can be approximated as a product state. In this analysis we shall look at how "distinguishable" our system actually is, and find a distance between wells where we can safely assume the particles to be distinguishable. \\ \\
To make an assessment of the systems distinguishability we will firstly look at the eigen-energies of the system given that we either:
\begin{itemize}
    \item Assume the particles to be distinguishable: \\ We construct the wavefunction as a product state,
    \begin{align*}
        \Psi(\mathbf{r}_1, \mathbf{r}_2) = \phi_L(\mathbf{r}_1)\otimes\phi_R(\mathbf{r}_2)
    \end{align*} 
    where $\phi_L$ and $\phi_R$ are the single-particle functions located in the left and right well respectivley. We then calculate the energy by diagonalizing the Hamiltonian matrix for this product state system. In such a product state system the Hamiltonian matrix becomes
    \begin{align*}
        H = H_L \otimes I + I \otimes H_R + V
    \end{align*}
    where $H_L$ and $H_R$ are the single-particle Hamiltonians for the left and right well, and $V$ is the interaction term between the two particles.
    \item Assume the particles to be indistinguishable, and construct a Slater determinant that we solve using Hartree-Fock theory.
\end{itemize}


If we are successful, we should find that for a certain distance between the wells, the "true" distinguishable energy will overlap with the indistinguishable HF-energy. Should this be the case, we can safely assume that the particles behave as distinguishable particles for our intents and purposes. 


\section{Optimization of the potential parameters}
As our goal is to realize single-qubit gates and the two-qubit iSwap gate, we need to find suitable configurations of our potential where we achieve the desired degeneracy in energy levels and also our desired level of entanglement between the two particles. As we've discussed in earlier sections the two configurations we are looking for are:
\begin{itemize}
    \item Config I: The measurement configuration, where all energy levels are distinct and there are minimal correlations between the two subsystems (particles). This corresponds to keeping all Von Neumann entropies in our system as close to zero as possible. With this, we know that our two-body energy eigenstates will have a product state structure, and maximal overlap with the Hartree product states $\ket{00}, \ket{01}, \ket{10}, \ket{11}$.
    \item Config II: The entangled configuration, where we have a degeneracy in the energy levels of our system. This degeneracy will give rise to an avoided crossings in the energy spectrum for the first and second energy eigenstates. In this configuration, the 1st and 2nd energy eigenstates are maximally entangled, while the other energy eigenstates are kept as pure as possible (product states). This corresponds to an entropy equal to 1 for $\ket{\phi_1}$ and $\ket{\phi_2}$, and 0 for $\ket{\phi_0}$ and $\ket{\phi_4}$. 
\end{itemize}
As an initial search, we make a grid search over the potential parameters, mostly to map out the landscape of the potential and find regions where we expect to find the desired configuations. Our grid search will be over the following parameters:
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Parameter & Range & Description \\
        \hline
        $D_l, D_r$ & [$D_{\text{min}}$, $D_{\text{max}}$] & The depth of the potential well \\
        $k_l, k_r$ & [$k_{\text{min}}$, $k_{\text{max}}$] & The width of the potential well \\
        $d$ & [$d_{\text{min}}$, $d_{\text{max}}$] & The distance between the two potential wells \\
        \hline
    \end{tabular}
    \caption{The parameters we will search over in our grid search}
\end{table}
with the constraint that $2 * D / \sqrt{k} < l$ where $l$ are the number of basis functions allocated to each well, as we've discussed earlier. This is to ensure that the basis functions are well within the potential well, and that we do not have any basis functions that are cut off by the potential. This then becomes a \emph{constrained optimization problem}. To perform our optimization we will use the \texttt{scipy.optimize} package, and the \texttt{minimize} function. We will use the \texttt{COBYQA} method, which is a derivative-free optimization method that is well suited for constrained optimization problems. This method is built on the concept of \emph{sequential quadratic programming}, that solves constrained, non-linear problems. For more details on the method, see \cite{razh_cobyqa}. The optimization will be done as follows:
\begin{itemize}
    \item Randomize an initial configuration of the parameters within the specified ranges.
    \item Run the optimization algorithm, with a high tolerance for convergence.
    \item Log 'good' regions where we achive the desired configurations.
    \item Initialize new configurations using the 'good' parameters found from the grid search.
    \item Run the optimization algorithm again, with a lower tolerance for convergenece.
\end{itemize}

\subsection{Objective function}
An objective function is constructed for the optimizer, which is the object to be minimized by our optimization algorithm. We will have two separate objective functions, one for each configuration we are looking for. Across both configurations, we want to minimize the $\zeta$-parameter, which directly controls the stability of time-evolution through the Hamiltonian matrix. The parameters is defined as 
\begin{equation*}
    \zeta = E_4 - E_1 - E_2 + E_0
\end{equation*}
and we can identify this as the \emph{phase} of the rotated time-evolution operator $U = e^{-iHt}$, expressed in terms of the energy eigenstates, $U = \sum_i^4 e^{-iE_it}\ket{\phi_i}\bra{\phi_i}$. We see that written in matrix form, this propagator is (where $\ket{11}$ is the 4th energy eigenstate)
\begin{align*}
    U = \begin{pmatrix}
        e^{-iE_{00}t} & 0 & 0 & 0 \\
        0 & e^{-iE_{01}t} & 0 & 0 \\
        0 & 0 & e^{-iE_{10}t} & 0 \\
        0 & 0 & 0 & e^{-iE_{11}t}
    \end{pmatrix}
\end{align*}
these eigenvectors are not unique, and we can always rotate them by a phase \textcolor{red}{(cite something here i guess?)}. The phase rotation puts an energy relation on the 4th eigenstate, namely $E_4 = E_1 + E_2 - E_0$. This relation means that the excitation energy from the ground state $\ket{00} \rightarrow \ket{11}$ is equal to the sum of excitation energies from $\ket{00}\rightarrow \ket{10}$ and $\ket{00}\rightarrow\ket{01}$, which ensures stability in our iSwap gate \textcolor{red}{Dette må skrives ut, og studeres mer! "This balances the excitation pathways??} \\\\
Configuration I, as we've mentioned, should have all energy levels distinct. This introduces penalties in our objective function corresponding to the overlap between energy eigenvalues and we want to Von Neumann entropies to be as close to zero as possible. In python we realise this in the following way
\begin{lstlisting}[language=Python]
target_entropy = np.zeros(4)
detuning_penalty = -min(0.5, np.abs(e_L - e_R))
entropy_penalty = np.linalg.norm(entropy - target_entropy)
ZZ_penalty = np.abs(E_4 - E_1 - E_2 + E_0)
\end{lstlisting}
where \texttt{e\_L} and \texttt{e\_R} are the energy levels of the left and right well, and \texttt{entropy} is the Von Neumann entropy of the subsystems. The \texttt{detuning\_penalty} is a penalty that is introduced to ensure that the energy levels are distinct, and the \texttt{entropy\_penalty} is a penalty that ensures that the entropy is as close to zero as possible. The objective function is then the sum of these two penalties. Furthermore, we would like the two configurations parameters to be close in parameter space, so that our evolution between the two configurations is smoooth. We introduce a penalty for this as well, similar to the \texttt{entropy\_penalty}.
\\ In configuration II we have a similar detuning penalty, but in this configuration we do want degeneracy in the 1st Hartree energy level in each well, as well as having an entropy penalty, but with a different target vector. 
\begin{lstlisting}[language=Python]
target_entropy = np.array([0, 1, 1, 0])
detuning_penalty = np.abs(e_L - e_R)
entropy_penalty = np.linalg.norm(entropy - target_entropy)
ZZ_penalty = np.abs(E_4 - E_1 - E_2 + E_0)
\end{lstlisting}
The objective function is then again the sum of these penalties. \\ 
The overall structure of the objective function is as follows:
\begin{itemize}
    \item Solve for the Hartree energy eigenstates of the system given the current parameters $\theta$.
    \item Calculate the reduced density matrices, and the Von Neumann entropies of the subsystems.
    \item Calculate the penalties for the current configuration, and return the sum of these penalties.
\end{itemize}




\section{Time-evolution}
\subsection{Simple system}
This simple system will allow us to study some of the most basic features of time-evolution, and entanglement, highlighting the avoided crossings in the energy spectrum for an interacting system. As was studied by Lander and Zener in the 1930s separately, the avoided crossing is a feature of the energy spectrum that arises when two energy levels are close in an interacting quantum system \\
As an initial exploration of the nuances of solving the time-dependent Schrödinger equation \eqref{eq:TDSE}, we will make a study of a simple two-level system Hamiltonian
\begin{equation}
    H(t) = H_0 + \lambda(t)H_I
\end{equation}
where the non-interacting, and interacting, Hamiltonians are given by
\begin{align*}
    H_0 &= \begin{pmatrix}
        E_0, 0 & 0, -E_0
    \end{pmatrix} \\
    H_I &= \begin{pmatrix}
        0, \Delta & \Delta, 0
    \end{pmatrix}
\end{align*}
This simple system will allow us to study some of the most basic features of time-evolution, and entanglement, highlighting the avoided crossings in the energy spectrum for an interacting system. As we are only dealing with a 2 by 2 matrix, direct diagonalization is not an issue not we we need to think much about computational resources, and it will be easy for us to test multiple algorithms without spending much computational power. With this simple system, we may test various time-evolution algorithms, and study stability and speed of the algorithms, to make an educated choice for our later study of a more complex, computationally demanding system.
\end{document}