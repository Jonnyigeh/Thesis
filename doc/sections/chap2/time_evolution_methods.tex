\documentclass{subfiles}
\begin{document}
%% TIME-EVOLUTION
\section{Time-evolution}
In this section, we describe the numerical implementation of the time-dependent Schrödinger equation (TDSE) for evolving quantum states under a (possibly time-dependent) Hamiltonian. Our primary goal is to simulate the time evolution of a quantum wavefunction from a known initial state, using various numerical methods to approximate both the time-evolution operator and the propagation of the wavefunction itself.

We begin by discussing how to discretize time for numerical propagation. We then introduce different strategies for approximating the time-evolution operator and solving the TDSE. Several integration schemes are examined, and we compare their accuracy, stability, and efficiency. We validate our implementation using the well-known Landau-Zener model before applying the methods to the more complex double-well Morse potential system.

\subsection{Time discretization}
The foundation of any numerical time-evolution scheme lies in the discretization of the time variable. Typically, this is done by introducing a time step $\Delta t$, and defining a grid of time points $t_n = n \Delta t$ where $n$ is an integer. In general, there are two ways to do this:
\begin{itemize}
    \item \textbf{Fixed time step:} This is the most common approach, where we define a fixed time step $\Delta t$ and evolve the system in small steps. 
    \item \textbf{Adaptive time step:} This approach allows for a variable time step, where the time step is adjusted based on the dynamics of the system. This can be useful for systems with rapidly changing dynamics, where a fixed time step may not be sufficient to capture the dynamics accurately.
\end{itemize}
In this work we will use primarily employing fixed time steps, as the system we are studying is not expected to exhibit rapidly changing dynamics, due to us being in control of how we vary the potential parameters and thus perturb the system. 


\subsection{Approximating the Time-evolution operator}
\textcolor{red}{TODO: We need to rephrase this section to be more general. We mention not only approximations to the time-evolution operator, but also numerical methods for solving the TDSE. We should also mention that we are not only interested in the time-evolution operator, but also in the wavefunction itself.} \\\\
As presented in section \ref{sec:time_evolution_theory}, the time-evolution operator is given as in \eqref{eq:time_evolution_operator}
\begin{align*}
    U(t, t_0) = \mathcal{T}\text{exp}\bigg(\frac{-i}{\hbar}\int_{t_0}^t H(t')dt'\bigg)
\end{align*}
which simplifies to $U(t) = e^{-iHt}$ for a time-independent system Hamiltonian. In either case, computing this operator exactly is often infeasible and we must, as we've discussed, resort to numerical approximations. Depending on the system size and whether the Hamiltonian is time-dependent, different numerical techniques are more appropriate. Below, we briefly outline the most common approaches considered in this work.
\begin{itemize}
    \item \textbf{Matrix exponentiation via Padé:} This is the most straightforward approach, where we compute a direct evaluation of the matrix exponential, typically using a Padé approximation combined with scaling and squaring, as implemented in \texttt{scipy.linalg.expm}. This method is efficient and works well for small to medium-sized matrices.
    \item \textbf{Taylor series expansion:} This method approximates the time-evolution operator as a Taylor series expansion in powers of the Hamiltonian. This is a very simple and straightforward method, but as we saw earlier, can be unstable or inaccurate unless the timestep is very small or the wavefunction norm is bounded.
    \item \textbf{Runge-Kutta methods:} These methods recast the TDSE as a first-order differential equation and integrate it stepwise. The 4th-order Runge-Kutta method (RK45) is common, though it does not preserve unitarity exactly.
    \item \textbf{Crank-Nicolson method:} An implicit midpoint method that approximates the evolution operator as a weighted average of states at the current and next time step. It preserves unitarity and is numerically stable, making it useful for stiff or rapidly varying systems.
\end{itemize}
In this work, we primarily implement matrix exponentiation and Crank-Nicolson, while briefly validating our results against RK45 and Euler-Cromer. The numerical stability, accuracy, and computational efficiency of each approach are evaluated in the context of our double-well Morse potential system. \\ \\
%% Matrix exponentiation
\subsubsection{Matrix exponentiation}
Following the material presented in section \ref{sec:time_evolution_theory}, we know that our time-evolution operator \eqref{eq:time_evolution_operator} is given numerically, as an approximation, by the following expression \eqref{eq:numerical_time_evolution_operator}
\begin{align*}
    U(t) \approx e^{-iH(t)\Delta t} 
\end{align*}
which is easily implemented direcly in python using the \texttt{scipy.linalg.expm} function. The approximation is only valid given a small enough\footnote{Small enough is system specific} $\Delta t$, and/or slowly varying Hamiltonian. This function uses the \texttt{scipy.linalg} package to compute the matrix exponential using the algorithm introduced in \cite{Al-Mohy_Higham_2010}, which in essence in a Padé approxmation to the exponential function, using a scaling and squaring method. The \texttt{scipy.linalg.expm} function is efficient and reliable, making it well-suited for systems of moderate size. 

The function is implemented as follows:
\begin{lstlisting}[language=Python]
def time_evolution_operator(H, t, dt):
    return scipy.linalg.expm(-1j * H(t) * dt)
time = [...]
psi = [...]
psi_t = np.zeros([...])
dt = time[1] - time[0] 
for t in time:
    U = time_evolution_operator(H, t, dt)
    psi_new = np.dot(U, psi)
    psi_t.append(psi_new)
    psi = psi_new
\end{lstlisting}
where \texttt{H} is the Hamiltonian matrix, \texttt{t} is the time, and \texttt{dt} is the time step. This code snippet outlines how the \texttt{expm} function is used to compute the time-evolution operator, and how this operator is used in a time loop to evolve the wavefunction \texttt{psi}. The time-evolution operator is computed at each time step, and the wavefunction is updated accordingly. The \texttt{psi\_t} variable is used to store the wavefunction at each time step, and can be used to visualize the dynamics of the wavefunction throughout the time-evolution procedure. \\ \\ 
While matrix exponentiation provides a robust method for computing the time-evolution operator, its accuracy for time-dependent Hamiltonians depends on how the Hamiltonian is evaluated at each time step. In particular, since $U(t, t_0) \approx \text{exp}(-iH(t)\Delta t)$, the choice of how to evaluate $H(t)$ on the interval $[t, t + \Delta t]$ can significantly affect the accuracy of the time-evolution operator. Some examples are
\begin{itemize}
    \item \textbf{Euler (explicit first order):} $U(t + \Delta t, t) = \text{exp}(-iH(t)\Delta t)$. This method is simple and fast, but inaccurate for rapidly varying Hamiltonians and/or large time steps. 
    \item \textbf{Midpoint evaluation (second order):} $U(t + \Delta t, t) = \text{exp}(-iH(t + \Delta t/2)\Delta t)$. This method is more accurate than the left-endpoint method, but still suffers from stability issues for large time steps.
    \item \textbf{Trapezoidal evaluation (second order, symmetric):} $U(t + \Delta t, t) = \text{exp}(-i(H(t) + H(t + \Delta t))/2 \Delta t)$. This method is more accurate than the midpoint method, and naturally leads into implicits methods like Crank-Nicolson. 
\end{itemize}
The choice of evaluation method depends on the specific system, and in this work, we primarily use the Euler method for simplicity and speed. \textcolor{red}{TODO: Add more details, possibly validate against the other two evaluation methods.} \\ In the next section we will discuss the different approach of using numerical integration methods to solve the time-evolution operator, namely the Crank-Nicolson method and the Runge-Kutta method. These methods are more general and can be used for time-dependent Hamiltonians, but are also more complex and computationally expensive.

\subsection{Numerical integrators}
As we have seen, direct matrix exponentiation provides a robust and unitary method for approximating the time-evolution operator. However, it becomes increasingly inefficient and impractical for high-dimensional systems, due to the computational complexity of evaluating the matrix exponential of the Hamiltonian in each time step. To address this, we resort to \emph{numerical integratorion methods},which offer more scalable alternatives for evolving the quantum system, and propagating the wavefunction through time.  \\\\ These numerical methods treat the Schrödinger equation \eqref{eq:TDSE} as a first-order differential equation in time and aim to approximate the solution iteratively over successive time steps. This foregoes the need to explicitly compute the time-evolution operator \eqref{eq:time_evolution_operator}, and instead focus on the evolution of the wavefunction itself. \\\\ 


Unlike direct matrix exponentiation - which preserves unitarity by construction - numerical integrators must be carefully designed to ensure that important physical properties, like norm conservation, are maintainted over time. Some integrators, like the Crank-Nicolson method, are specifically designed to preserve unitarity and are well-suited for long-time simulations. Others, like the Runge-Kutta method, may not preserve unitarity exactly, but is more flexible and can provide highly accurate results for a wide range of systems, and for shorter time simulations. \\ \\ In this section, we present and suggest procedures to implement two widely used numerical integration schemes:
\begin{itemize}
    \item \textbf{Crank-Nicolson method:} A semi-implicit, second-order finite-difference method that, while not computing the matrix exponential explicitly, it yields an approximative time-evolution operator as a weighted average of states at the current and next time step. It preserves unitarity by construction and is numerically stable, making it useful for rapidly varying systems. The unitary nature of the method ensures that the wavefunction remains normalized over time.
    \item \textbf{Runge-Kutta method (RK4):} A fourth-order explicit integration method that offers high accuracy and ease of implementation. It is particularly useful for systems with smooth dynamics, but does not preserve unitarity exactly. The RK4 method is computationally efficient, and instead focuses on evolving the wavefunction iteratively over time.
\end{itemize}

These methods are implemented in Python, and we will provide code snippets to illustrate their usage. They offer a powerful and flexible alternative framework for simulating the time evolution of quantum systems, especially when direct matrix exponentiation becomes impractical. The choice of method depends on the trade-off between computational cost, accuracy, and the specific requirements of the system being studied. In the following sections, we will present the implementation details and performance of these methods in the context of the simple Landau-Zener system\eqref{eq:landau_zener} and, later on, employ these methods to study the full double-well Morse potential system.

%% CRANK-NICOLSON
\subsubsection{Crank-Nicolson method} 
Building on the limitations of the simple method of Euler-Cromer, and to improve upon the computational cost of direct matrix exponentiation, the \emph{Crank-Nicolson method} offers a compelling, and physically grounded alternative. As mentioned, this is a semi-implicit, second-order finite-difference method that aims to solve the TDSE\eqref{eq:TDSE} iteratively over time. The weighted average of states used makes an approximative time-evolution operator - which in turn makes the method unitary by construction - and thus is key to preserve essential physical properties like \textbf{norm conservation} and \textbf{unitarity}. \\\\
At its core, the Crank-Nicolson method applies the trapezoidal rule to the Schrödinger equation, treating the Hamiltonian propagator as an average of its behaviour at the beginning and end of each time interval. This in turn leads to scheme that does not explicitly compute the time-evolution operator, but still approximates its action through a symmetric update rule. As stated, we apply the trapezoidal rule to the time-derivative in the TDSE\eqref{eq:TDSE}:
\begin{align*}
    \frac{\ket{\Psi(t + \Delta t)} - \ket{\Psi(t)}}{\Delta t} \approx \frac{-i}{2\hbar}\bigg[H(t + \Delta t)\ket{\Psi(t + \Delta t)} - H(t)\ket{\Psi(t)}\bigg]
    \ket{\Psi(t + \Delta t)} \approx \ket{\Psi(t)} - \frac{i\Delta t}{2\hbar}\bigg[H(t + \Delta t)\ket{\Psi(t + \Delta t)} + H(t)\ket{\Psi(t)}\bigg]
\end{align*}
Rearranging the terms of this equation, yields the Crank-Nicolson update rule:
\begin{align*}
    \bigg[\mathbb{I} + \frac{i\Delta t}{2\hbar}H(t + \Delta t)\bigg]\ket{\Psi(t + \Delta t)} = \bigg[\mathbb{I} - \frac{i\Delta t}{2\hbar}H(t)\bigg]\ket{\Psi(t)}
\end{align*}
This equation represents a \textbf{linear system} of equations for the wavefunction, that must be solved at each time step. The implicit nature contributes to the stability of the method, and the symmetric structure of the update rule ensures that the resulting time-evolution operator is unitary to second order in time $\Delta t$. Expressed in matrix form, by left-multiplying the inverse of the left-hand matrix, the update rule, or rather, the approximative time-evolution operator $U_{CN}$, can be written as:
\begin{align*}
    U_{CN} (\Delta t) = \bigg[\mathbb{I} + \frac{i\Delta t}{2\hbar}H(t + \Delta t)\bigg]^{-1}\bigg[\mathbb{I} - \frac{i\Delta t}{2\hbar}H(t)\bigg]
\end{align*}
This expression is a \textbf{Padé approximation} to the exponential function\textcolor{red}{TODO: Add reference}, and it provides a second-order accurate approximation to the time-evolution operator that is norm-preserving. In the time-independent case, it acts as a rational approximation\textcolor{red}{TODO: Add more details, and references to this} to the exponential time-evolution operator,
\begin{align*}
    U = e^{-iH\Delta t/\hbar} \approx \frac{\mathbb{I} - \frac{i\Delta t}{2\hbar}H}{\mathbb{I} + \frac{i\Delta t}{2\hbar}H}
\end{align*}
Viewed as such, the Crank-Nicolson method situates itself somewhere in between numerical integration methods and approximate exponentiaion schemes, providing a physical, flexible and efficient framework for quantum state time-evolution. Some of the key advantages of the Crank-Nicolson method include:
\begin{itemize}
    \item \textbf{Unitarity:} The method preserves the norm of the wavefunction, ensuring that the total probability remains constant over time. This is crucial for maintaining physical consistency in quantum simulations.
    \item \textbf{Stability:} The implicit nature of the method provides numerical stability, making it particularly suitable for stiff or rapidly varying systems - or long-time simulations.
    \item \textbf{Accuracy:} The second-order accuracy of the method supports the use of larger time steps compared to first-order methods, while still maintaining a high level of precision, as the local error scales as $\mathcal{O}(\Delta t^3)$, and global error as $\mathcal{O}(\Delta t^2)$.
    \item \textbf{Sparsity:} The method can be efficiently implemented for sparse Hamiltonians, as solving the linear system is generally more efficient than computing the full matrix exponential.
\end{itemize}
In the following, we shall outline a suggested procedure for implementing the Crank-Nicolson method in Python, and provide a code snippet to illustrate its usage. The implementation is straightforward, and involves the following steps: \textcolor{red}{TODO: Either make pseudo-code, or skip altoghter?}
\begin{itemize}
    \item Define the Hamiltonian matrix $H(t)$ at the current time step.
    \item Construct the Crank-Nicolson update operator $U_{CN}(\Delta t)$ using the Hamiltonian.
    \item Solve the linear system of equations to obtain the wavefunction at the next time step $\ket{\Psi(t + \Delta t)}$.
    \item Repeat the process for each time step, updating the wavefunction iteratively.
\end{itemize}
\begin{lstlisting}[language=Python]
N = ... # Number of time steps
hbar = 1.0  # Reduced Planck's constant (in appropriate units)
psi = np.zeros([...])  # Initial wavefunction
time = np.linspace(0, 10, N)  # Time grid
dt = time[1] - time[0]  # Time step
def H(t):
    # Define the Hamiltonian matrix at time t
    # This is a placeholder function and should be replaced with the actual Hamiltonian
    return np.array([[1, 0], [0, 1]])  # Example: Identity matrix

def crank_nicolson(H, psi, t, dt):
    # Define the Hamiltonian matrix at the current time step
    H_current = H(t)
    H_next = H(t + dt)
    
    # Construct the Crank-Nicolson update operator
    U_CN = np.linalg.inv(np.eye(len(psi)) + 1j * dt / (2 * hbar) * H_next) @ \
           (np.eye(len(psi)) - 1j * dt / (2 * hbar) * H_current)
    
    # Solve the linear system to obtain the wavefunction at the next time step
    psi_new = np.dot(U_CN, psi)
    
    return psi_new
# Time evolution loop
psi_t = np.zeros((N, len(psi)), dtype=complex)  # Store wavefunction at each time step
for t in time:
    psi_new = crank_nicolson(H, psi, t, dt)
    psi_t.append(psi_new)
    psi = psi_new  # Update the wavefunction for the next iteration
\end{lstlisting}
\\\\


\subsubsection{Runge-Kutta method}
In contrast to the Crank-Nicolson method, the Runge-Kutta method trades off unitarity and exact norm preservation for simplicity, flexibility and short-term accuracy. As such, it provides a versatile and widely used tool for initial value problems, and is particularly useful for systems with smooth dynamics. The RK4 method, in particular, is a fourth-order explicit integration scheme that offers high accuracy and ease of implementation.
\\\\
Returning to the TDSE\eqref{eq:TDSE}, we can recast it as a first-order differential equation (ODE) in time:
\begin{align*}
    \frac{\partial}{\partial t}\ket{\Psi(t)} = -\frac{i}{\hbar}H(t)\ket{\Psi(t)}
\end{align*}
The Runge-Kutta methods are designed to solve ODEs of the form $\frac{dy}{dt} = f(t, y)$, where $y$ is the dependent variable (in our case, the wavefunction $\ket{\Psi(t)}$) and $f(t, y)$ is a function that describes the dynamics of the system (i.e the Hamiltonian). It does so by approximating the solution at successive time steps using a weighted average of the derivatives (slopes) evaluated at different intermediate points within the time step. The RK4 method, in particular, uses four evaluations of the derivative to achieve fourth-order accuracy, and provides a balanced trade-off between accuracy and computational efficiency and is a common choice in quantum dynamics when unitarity is not strictly required. The RK4 update rule is given by:
\begin{align*}
    k_1 &= f(t, \Psi(t))) = -\frac{i}{\hbar}H(t)\ket{\Psi(t)} \\
    k_2 &= f(t + \frac{\Delta t}{2}, \Psi(t) + \frac{\Delta t}{2}k_1) \\
    k_3 &= f(t + \frac{\Delta t}{2}, \Psi(t) + \frac{\Delta t}{2}k_2) \\
    k_4 &= f(t + \Delta t, \Psi(t) + \Delta t k_3)
\end{align*}
which yields the update to the wavefunction as
\begin{align*}
    \Psi(t + \Delta t) = \Psi(t) + \frac{\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4)
\end{align*}
This method achieves a local error of $\mathcal{O}(\Delta t^5)$ and a global error of $\mathcal{O}(\Delta t^4)$, making it highly accurate for sufficiently small time steps $\Delta t$. However, as we've mentioned, the RK4 method does not infact preserve unitarity and thus the wavefunction norm may drift, especially over longer time scales, unless norm corrections are applied repeatedly. Despite these limitations, for short time simulations, the RK4 remains a robust choice for time-evolution. Summarizing the key advantages of the RK4 method:
\begin{itemize}
    \item \textbf{Simplicity:} The RK4 method is easy to implement and understand, making it a popular choice for many applications.
    \item \textbf{Flexibility:} The method can be applied to a wide range of systems, including those with time-dependent Hamiltonians.
    \item \textbf{Explicitness:} The RK4 method is explicit, meaning that it does not require the solution of a linear system at each time step, making it computationally efficient for a wide range of systems.
    \item \textbf{Accuracy:} The fourth-order accuracy enables the use of relatively large time steps while maintaining a high level of accuracy.
\end{itemize}
We will now outline a suggestion for implementing a numerical RK4 method in Python, and provide a code snippet to illustrate its usage. 
\begin{itemize}
    \item Define the Hamiltonian matrix $H(t)$ at the current time step (RHS of ODE).
    \item Compute the four slopes $k_1$, $k_2$, $k_3$, and $k_4$ using the Hamiltonian.
    \item Construct the RK4 update operator using the slopes.
    \item Update the wavefunction iteratively using the RK4 update rule.
    \item Repeat the process for each time step, updating the wavefunction iteratively.
    \item Optionally, apply norm corrections to maintain unitarity.
\end{itemize}
\begin{lstlisting}[language=Python]
psi = np.zeros([...])  # Initial wavefunction
N = ... # Number of time steps
time = np.linspace(0, 10, N)  # Time grid
dt = time[1] - time[0]  # Time step
hbar = 1.0  # Reduced Planck's constant (in atomic units)
def H(t):
    # Define the Hamiltonian matrix at time t
    # This is a placeholder function and should be replaced with the actual Hamiltonian
    return np.array([[1, 0], [0, 1]])  # Example: Identity matrix

def runge_kutta_4(H, psi, t, dt):
    # Define the Hamiltonian matrix at the current time step
    H_current = H(t)
    H_dt2 = H(t + dt / 2)
    H_dt = H(t + dt)
    
    # Compute the four slopes
    k1 = -1j / hbar * np.dot(H_current, psi)
    k2 = -1j / hbar * np.dot(H_dt2, psi + dt / 2 * k1)
    k3 = -1j / hbar * np.dot(H_dt2, psi + dt / 2 * k2)
    k4 = -1j / hbar * np.dot(H_dt, psi + dt * k3)

    # Update the wavefunction using the RK4 update rule
    psi_new = psi + dt / 6 * (k1 + 2 * k2 + 2 * k3 + k4)
    return psi_new

# Time evolution loop
psi_t = np.zeros((N, len(psi)), dtype=complex)  # Store wavefunction at each time step
for t in time:
    psi_new = runge_kutta_4(H, psi, t, dt)  
    # Optionally, apply norm corrections to maintain unitarity
    norm_correction = np.linalg.norm(psi_new)
    psi_new /= norm_correction  # Normalize the wavefunction
    psi_t.append(psi_new)
    psi = psi_new  # Update the wavefunction for the next iteration
\end{lstlisting}
This code snippet outlines a suggested procedure, however - in our work, we shall instead use the \texttt{scipy.integrate} packages \texttt{RK45}\footnote{See \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.RK45.html}} method, which is a built-in implementation of the RK4 method. This package provides a convenient and efficient way to perform numerical integration of ODEs, and is well-suited for our purposes.  \\\\
In the next section, we will compare the performance and qualitative behaviour of the Crank-Nicolson and RK4 methods when applied to the Landau-Zener system \eqref{eq:landau_zener}, providing insight into their respective advantages and limitations in a controlled testbed scenario.

    %% Simple system
\subsection{Numerical validation: Landau-Zener model}
To systematically validate our implementation of the aforementiond numerical methods, we require a simple yet representative quantum system that exhibits non-trivial dynamics but still has closed form solutions for comparison. For this purpose, we will once again turn to the simple Landau-Zener model \eqref{eq:landau_zener}, introduced in the section on avoided crossings \ref{sec:avoided_crossings}. This simple two-level system is governed by a time-dependent Hamiltonian. It captures the essential features of quantum dynamics, making it an ideal testbed for our numerical methods in preparation for the more complex double-well Morse potential system we shall study. \\\\

Once again, the Hamiltonian for the Landau-Zener model is given by:
\begin{align*}
    H(t) = \begin{pmatrix}
        vt & V \\
        V & -vt
\end{pmatrix}
\end{align*}
where $v$ is the sweeping velocity, $V$ is the coupling strength, and $t$ is the time. 

At $t=0$, the uncoupled system would have a degeneracy, but the coupling $V$ opens a gap (lifts the degeneracy), resulting in an avoided crossing as we saw in section \ref{sec:avoided_crossings}. Thus, this minimal system captures one of the simplest non-trivial dyanmics of a quantum system which also leads to a well known analytical solution for the transition probability \eqref{eq:landau_zener_trans_prob}. 

This system is particularly useful for validating our numerical methods, and serves as a suitable testbed for benchmarking due to several reasons, of which some are listed below:
\begin{itemize}
    \item \textbf{Low dimensionality:} The Landau-Zener model is a two-level system, making it computationally efficient to simulate and analyze. This allows us to focus on the numerical methods without being overwhelmed by the computational complexity of the system.
    \item \textbf{Non-trivial dynamics:} The system exhibits interesting dynamics, such as transitions betweeen states, which require the numerical implementations to accurately capture the time evolution of the wavefunction. This provides a meaningful test for the accuracy and stability of the numerical methods.
    \item \textbf{Analytical reference:} The Landau-Zener model has well-known analytical solutions for the transition probabilities and wavefunction dynamics, allowing us to directly compare the results of our numerical methods against these exact solutions. This serves as a benchmark for assessing the performance of the numerical methods.
\end{itemize}

To test the performance of our numerical implementations, we will evolve an initial state $\ket{\Psi(0)} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$, which corresponds to the ground state of the uncoupled system, from an initial time $t_0<0$ to a final time $t_f>0$. We will then compare:
\begin{itemize}
    \item The performance of each integrator by
    \begin{itemize}
        \item Global error vs. time step $\Delta t$.
        \item Runtime efficiency
        \item Stability and accuracy under varying conditions. 
    \end{itemize}
    \item We will also qualitatively investigate how the various methods behave near the avoided crossing. \textcolor{red}{TODO: will we do this?}
\end{itemize}
\\ \\ We numerically propagate an initial wavefunction prepared in the ground state of the uncoupled system (the \emph{diabatic basis}), and compute the final transition probability $P_{12}$, which is defined as the probability of finding the system in the excited state at time $t_f$, given analytically by \eqref{eq:landau_zener_trans_prob}. \\

The system is initialized in the state $$\ket{\Psi(0)} = \ket{\Psi_1} = \begin{pmatrix} 1 \\ 0 \end{pmatrix},$$ and we compute the final transition probability $$P_{12} = |\langle \Psi(t_f) | \Psi_2 \rangle|^2,$$ where $\ket{\Psi_2}$ is the excited diabatic state of the system. This process is repeated for various numerical methods using a fixed time step $\Delta t$, while varying the length of simulation $2T$. The Landau-Zener transition probability is derived in the infinite time-limit, i.e $t\rightarrow \pm \infty$, this analysis illustrate how each numerical method converges to the analytical solution as the simulation time increases.
\begin{figure}{h!}
\centering
\includegraphics[width=1.0\textwidth]{figs/landau_zener_convergence_benchmark.pdf}
\caption{Final transition probability computed using various numerical methods for increasing total simulation time $
2T$. All methods converge toward the analytical Landau-Zener probability (dashed line), with RK4 showing good accuracy until instability sets in at longer times due to lack of normalization. Euler becomes unstable very early. The x-axis is in logarithmic scale.}
\end{figure}

As we can see in figure \eqref{fig:landau_zener_convergence_benchmark}, all methods used converge to the analytical solution, but not exactly. The two Taylor-expansion methods (Euler and RK4) show good agreement but quickly become unstable, with Euler diverging first, as we've seen already in figure \eqref{fig:landau_zener}. Both of these methods can be improved upon by introducing norm corrections, at a computational cost (albeit minimal). The Crank-Nicolson method, on the other hand, remains stable and accurate for longer simulation times, as it preserves unitarity by construction. 
\\ \\ 

To assess the computational performance of the numerical methods, a full Landau-Zener simulation was measured using the \texttt{time.perf\_counter()} function in Python, which provides a high-resolution timer suitable for short time measurements.This measurement also includes system overhead, yielding realistic performance metrics to evaluate each method. 

The benchmark was performed for the two-level system with sweeping parameter $v=7.0$, coupling strength $V=1.0$, and a time integration interval of $t \in [-5, 5]$ with $100{,}000$ time steps, i.e a time-step of $\Delta t = 10^{-4}$s. The results are presented in the following table \eqref{tab:landau_zener_runtime}, which summarizes the runtime of each method, along with remarks on their performance characteristics.

\begin{table}[h!]
\centering
\caption{Runtime comparison of time propagation methods for the Landau-Zener model ($v = 7.0$, $\Delta = 1.0$, $t \in [-5, 5]$, 100,000 steps). Timing measured using \texttt{time.perf\_counter()}.}
\begin{tabular}{l c l}
\toprule
\textbf{Method} & \textbf{Time (ms)} & \textbf{Remarks} \\
\midrule
Matrix Exponential & 5942.1 & Most accurate, but very slow \\
Euler              & 828.6  & Fastest, but unstable \\
RK4                & 2621.0 & Balanced performance \\
Crank--Nicholson   & 1953.6 & Stable and efficient \\
\bottomrule
\end{tabular}\label{tab:landau_zener_runtime}
\end{table}
We see here that the more accurate, direct matrix exponentiation method is the slowest by a good margin, while the Euler method is the

\subsection{Ramping protocols}
\textcolor{red}{TODO: Add a section on ramping protocols, and how to implement them in the context of the time-evolution methods. This is important for the double-well Morse potential system, where we will need to ramp the potential parameters to avoid non-adiabatic transitions.} \\\\
\end{document}